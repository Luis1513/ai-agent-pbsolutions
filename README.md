# ğŸ¤– PB RAG - Agente de IA con RAG para Punta Blanca Solutions

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un **agente de IA inteligente** que demuestra capacidades de **RAG (Retrieval-Augmented Generation)** utilizando tecnologÃ­as de Google Cloud Platform y LangGraph. El agente puede responder preguntas sobre Punta Blanca Solutions basÃ¡ndose en informaciÃ³n extraÃ­da de su sitio web y LinkedIn.

### ğŸ¯ Objetivo Principal
Crear un agente de IA que pueda:
1. **Recibir preguntas** a travÃ©s de una API REST
2. **Buscar informaciÃ³n relevante** en una base de conocimiento vectorial
3. **Generar respuestas** utilizando un LLM con el contexto recuperado
4. **Responder en formato JSON estructurado** con fuentes y nivel de confianza

## ğŸ—ï¸ Arquitectura del Sistema

### Arquitectura General
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Node    â”‚â”€â”€â”€â–¶â”‚  Retrieval Node  â”‚â”€â”€â”€â–¶â”‚ Generation Node â”‚â”€â”€â”€â–¶â”‚  Output Node    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ ValidaciÃ³n    â”‚    â”‚ â€¢ Embeddings     â”‚    â”‚ â€¢ LLM Prompt    â”‚    â”‚ â€¢ Formato JSON  â”‚
â”‚ â€¢ Limpieza      â”‚    â”‚ â€¢ Pinecone Query â”‚    â”‚ â€¢ Context Build â”‚    â”‚ â€¢ Respuesta     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos
1. **RecepciÃ³n**: API REST recibe pregunta del usuario
2. **ValidaciÃ³n**: Se valida y limpia la pregunta
3. **BÃºsqueda**: Se genera embedding y se busca en Pinecone
4. **GeneraciÃ³n**: Se construye contexto y se genera respuesta con LLM
5. **Formato**: Se estructura la respuesta en JSON con metadatos

## ğŸ› ï¸ Stack TecnolÃ³gico

### TecnologÃ­as Principales
- **FastAPI**: Framework web moderno y rÃ¡pido para la API REST
- **LangGraph**: OrquestaciÃ³n del agente con flujo de trabajo definido
- **OpenAI GPT-4o-mini**: LLM para generaciÃ³n de respuestas
- **OpenAI text-embedding-3-small**: Modelo de embeddings optimizado
- **Pinecone**: Base de datos vectorial para bÃºsqueda semÃ¡ntica
- **Python 3.11**: VersiÃ³n estable y compatible con todas las dependencias

### Dependencias de Desarrollo
- **Black**: Formateador de cÃ³digo Python
- **Ruff**: Linter rÃ¡pido para Python
- **Pytest**: Framework de testing

## ğŸ“ Estructura del Proyecto

```
ai-agent/

â”œâ”€â”€ app/                          # AplicaciÃ³n principal
â”‚   â”œâ”€â”€ api/                     # Endpoints de la API
â”‚   â”‚   â””â”€â”€ main.py             # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ core/                    # ConfiguraciÃ³n central
â”‚   â”‚   â””â”€â”€ settings.py         # GestiÃ³n de variables de entorno
â”‚   â””â”€â”€ graph/                   # Agente LangGraph
â”‚       â””â”€â”€ agent_graph.py      # ImplementaciÃ³n del agente RAG

â”œâ”€â”€ data/                        # Datos procesados
â”‚   â”œâ”€â”€ *.json                  # Documentos originales de Punta Blanca
â”‚   â”œâ”€â”€ processed_chunks.json   # Chunks de texto procesados
â”‚   â””â”€â”€ embeddings_processed.json # Embeddings generados

â”œâ”€â”€ ingest/                      # Pipeline de procesamiento
â”‚   â”œâ”€â”€ document_processor.py   # Procesamiento de documentos
â”‚   â”œâ”€â”€ embedding_processor.py  # GeneraciÃ³n de embeddings
â”‚   â”œâ”€â”€ pinecone_uploader.py    # Carga a Pinecone
â”‚   â””â”€â”€ run_pipeline.py         # Orquestador del pipeline

â”œâ”€â”€ scripts/                     # Scripts de utilidad
â”‚   â””â”€â”€ smoke_check.py          # VerificaciÃ³n de configuraciÃ³n

â”œâ”€â”€ Dockerfile                   # ContainerizaciÃ³n
â”œâ”€â”€ .dockerignore               # Archivos a excluir del Docker
â”œâ”€â”€ requirements.txt             # Dependencias de Python
â””â”€â”€ README.md                    # Este archivo
```

## ğŸ”§ Decisiones TÃ©cnicas y Justificaciones

### 1. **ElecciÃ³n de OpenAI como LLM**
**DecisiÃ³n**: Uso de GPT-4o-mini y text-embedding-3-small
**JustificaciÃ³n**: 
- **CrÃ©ditos existentes**: Ya disponÃ­a de crÃ©ditos en OpenAI, representando costo cero
- **Calidad probada**: GPT-4o-mini ofrece excelente relaciÃ³n calidad-precio
- **Consistencia**: Mismo proveedor para embeddings y generaciÃ³n asegura compatibilidad
- **API estable**: OpenAI tiene una de las APIs mÃ¡s estables y documentadas

### 2. **Modelo de Embeddings Small**
**DecisiÃ³n**: text-embedding-3-small en lugar de text-embedding-3-large
**JustificaciÃ³n**:
- **OptimizaciÃ³n de costos**: 50% menos costoso que el modelo large
- **Dimensiones adecuadas**: 1536 dimensiones son suficientes para bÃºsqueda semÃ¡ntica
- **Performance**: Mantiene excelente calidad para casos de uso de RAG
- **Velocidad**: GeneraciÃ³n mÃ¡s rÃ¡pida de embeddings

### 3. **TamaÃ±o de Chunks Optimizado**
**DecisiÃ³n**: Chunk size de 750 caracteres con overlap de 150
**JustificaciÃ³n**:
- **Contexto preservado**: 750 caracteres permiten mantener oraciones completas
- **Overlap estratÃ©gico**: 150 caracteres aseguran continuidad entre chunks
- **Balance memoria-calidad**: Optimiza uso de tokens del LLM
- **Evita cortes abruptos**: Previene pÃ©rdida de contexto importante

### 4. **Arquitectura de Carpetas data/ingest**
**DecisiÃ³n**: SeparaciÃ³n clara entre datos crudos y pipeline de procesamiento
**JustificaciÃ³n**:
- **Datos estructurados**: Los JSONs contienen informaciÃ³n limpia y organizada
- **Pipeline reutilizable**: El proceso de ingest puede aplicarse a nuevos datos
- **SeparaciÃ³n de responsabilidades**: Datos vs. lÃ³gica de procesamiento
- **Facilita mantenimiento**: Estructura clara para futuras expansiones

### 5. **Uso de Pinecone como Vector Store**
**DecisiÃ³n**: Pinecone en lugar de alternativas gratuitas como Chroma o FAISS
**JustificaciÃ³n**:
- **Escalabilidad**: Maneja grandes volÃºmenes de vectores eficientemente
- **Reranking avanzado**: Implementa BGE-reranker-v2-m3 para mejor relevancia
- **API robusta**: Interfaz estable y bien documentada
- **IntegraciÃ³n nativa**: Funciona perfectamente con LangChain/LangGraph

### 6. **ImplementaciÃ³n de Reranking**
**DecisiÃ³n**: Uso de BGE-reranker-v2-m3 en Pinecone
**JustificaciÃ³n**:
- **Mejora significativa**: Reranking puede mejorar la relevancia en 20-30%
- **Costo-beneficio**: El costo adicional se compensa con mejor calidad de respuestas
- **ImplementaciÃ³n nativa**: Pinecone lo maneja automÃ¡ticamente
- **Sin complejidad adicional**: No requiere lÃ³gica adicional en el cÃ³digo

### 7. **Arquitectura LangGraph con 4 Nodos**
**DecisiÃ³n**: ImplementaciÃ³n secuencial simple en lugar de flujo complejo
**JustificaciÃ³n**:
- **Simplicidad**: SoluciÃ³n que funciona vs. complejidad innecesaria
- **Debugging fÃ¡cil**: Cada nodo tiene responsabilidad Ãºnica
- **Mantenibilidad**: FÃ¡cil de entender y modificar
- **Escalabilidad**: FÃ¡cil agregar nuevos nodos o modificar flujo

### 8. **FastAPI como Framework Web**
**DecisiÃ³n**: FastAPI en lugar de Flask o Django
**JustificaciÃ³n**:
- **Performance**: Rendimiento superior para APIs
- **Type hints**: ValidaciÃ³n automÃ¡tica con Pydantic
- **DocumentaciÃ³n automÃ¡tica**: Swagger/OpenAPI generado automÃ¡ticamente
- **Async support**: Preparado para operaciones asÃ­ncronas futuras

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos
- Python 3.11+
- Docker (para containerizaciÃ³n)
- Cuenta en OpenAI con API key
- Cuenta en Pinecone con API key

### Variables de Entorno
Crear archivo `.env` en la raÃ­z del proyecto:
```bash
# OpenAI Configuration
OPENAI_API_KEY=tu_api_key_aqui
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Pinecone Configuration
PINECONE_API_KEY=tu_api_key_aqui
PINECONE_INDEX=agent-db
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Application Configuration
ENV=dev
```

### InstalaciÃ³n Local
```bash
# Clonar repositorio
git clone https://github.com/[tu-usuario]/ai-agent-pbsolutions.git
cd ai-agent-pbsolutions

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tus API keys

# Ejecutar pipeline de ingest
python -m ingest.run_pipeline

# Ejecutar aplicaciÃ³n
uvicorn app.api.main:app --reload
```

### InstalaciÃ³n con Docker
```bash
# Construir imagen
docker build -t pb-rag .

# Ejecutar contenedor
docker run -p 8000:8000 --env-file .env pb-rag
```

## ğŸ“Š Pipeline de Datos

### 1. **Procesamiento de Documentos**
```bash
python -m ingest.document_processor
```
- Lee archivos JSON de la carpeta `data/`
- Divide texto en chunks de 750 caracteres
- Mantiene metadatos (fuente, secciÃ³n, ID)

### 2. **GeneraciÃ³n de Embeddings**
```bash
python -m ingest.embedding_processor
```
- Genera embeddings para cada chunk
- Usa OpenAI text-embedding-3-small
- Almacena embeddings en formato JSON

### 3. **Carga a Pinecone**
```bash
python -m ingest.pinecone_uploader
```
- Carga embeddings a Pinecone en lotes
- Configura Ã­ndice con dimensiones correctas
- Verifica carga exitosa

### 4. **Pipeline Completo**
```bash
python -m ingest.run_pipeline
```
- Ejecuta todo el proceso secuencialmente
- Proporciona feedback en tiempo real
- Maneja errores y continÃºa el proceso

## ğŸ” Uso de la API

### Endpoint Principal: `/ask`
**POST** `/ask`

**Request Body:**
```json
{
  "question": "Â¿QuÃ© servicios ofrece Punta Blanca?"
}
```

**Response:**
```json
{
  "answer": "Punta Blanca ofrece servicios de consultorÃ­a en IA, desarrollo de soluciones a medida...",
  "sources": [
    "https://www.puntablanca.ai/services",
    "https://www.puntablanca.ai/"
  ],
  "confidence": 0.85
}
```

### Health Check: `/healthz`
**GET** `/healthz`

**Response:**
```json
{
  "status": "ok",
  "env": "dev",
  "openai_model": "gpt-4o-mini",
  "pinecone_index": "agent-db"
}
```

### Ejemplos de Uso

#### Con curl
```bash
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "Â¿QuiÃ©nes son los fundadores de Punta Blanca?"}'
```

#### Con Python
```python
import requests

response = requests.post(
    "http://localhost:8000/ask",
    json={"question": "Â¿QuÃ© es AI Fast Track?"}
)

print(response.json())
```

## ğŸ§ª Testing y VerificaciÃ³n

### Smoke Test
```bash
python scripts/smoke_check.py
```
Verifica:
- ConexiÃ³n con OpenAI
- ConexiÃ³n con Pinecone
- CreaciÃ³n/configuraciÃ³n de Ã­ndice
- Operaciones bÃ¡sicas de vectores

### Tests Unitarios
```bash
pytest test/
```

### VerificaciÃ³n Manual
1. Ejecutar aplicaciÃ³n
2. Hacer pregunta de prueba
3. Verificar respuesta coherente
4. Verificar fuentes y confianza

## ğŸš€ Deployment en Cloud Run

### 1. **Construir y Taggear Imagen**
```bash
# Construir imagen
docker build -t pb-rag .

# Taggear para Google Container Registry
docker tag pb-rag gcr.io/[PROJECT-ID]/pb-rag

# Subir a GCR
docker push gcr.io/[PROJECT-ID]/pb-rag
```

### 2. **Deploy en Cloud Run**
```bash
gcloud run deploy pb-rag \
  --image gcr.io/[PROJECT-ID]/pb-rag \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars="OPENAI_API_KEY=[KEY],PINECONE_API_KEY=[KEY]"
```

### 3. **Verificar Deployment**
```bash
# Obtener URL del servicio
gcloud run services describe pb-rag --region us-central1

# Probar endpoint
curl -X POST "[URL]/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "Test question"}'
```

## ğŸ“š Recursos y Referencias

### DocumentaciÃ³n Oficial
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Pinecone Documentation](https://docs.pinecone.io/)

### Tutoriales y Ejemplos
- [LangGraph Agent Examples](https://github.com/langchain-ai/langgraph/tree/main/examples)
- [FastAPI Deployment Guide](https://fastapi.tiangolo.com/deployment/)
- [Google Cloud Run Quickstart](https://cloud.google.com/run/docs/quickstarts)

### Herramientas de Desarrollo
- [Postman](https://www.postman.com/) - Testing de API
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) - ContainerizaciÃ³n
- [Google Cloud CLI](https://cloud.google.com/sdk/docs/install) - Deployment

## ğŸ¤ ContribuciÃ³n

### Estructura de Commits
- `feat:` Nueva funcionalidad
- `fix:` CorrecciÃ³n de bugs
- `docs:` DocumentaciÃ³n
- `refactor:` RefactorizaciÃ³n de cÃ³digo
- `test:` Tests y testing

### Pull Request Process
1. Fork del repositorio
2. Crear rama feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -m 'feat: agregar nueva funcionalidad'`
4. Push a rama: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto es parte de una prueba tÃ©cnica para el puesto de AI Engineer en Punta Blanca Solutions.

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado como prueba tÃ©cnica para demostrar capacidades en:
- ImplementaciÃ³n de RAG con LangGraph
- IntegraciÃ³n de APIs de IA (OpenAI, Pinecone)
- Desarrollo de APIs REST con FastAPI
- ContainerizaciÃ³n y deployment en Google Cloud Platform
- Arquitectura de sistemas de IA escalables

---

**Nota**: Este proyecto estÃ¡ optimizado para funcionar dentro de los lÃ­mites gratuitos de GCP y utiliza eficientemente los crÃ©ditos disponibles en OpenAI para minimizar costos operativos.
