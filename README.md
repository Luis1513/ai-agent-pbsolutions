# ü§ñ PB RAG - Agente de IA con RAG para Punta Blanca Solutions

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un **agente de IA inteligente** que demuestra capacidades de **RAG (Retrieval-Augmented Generation)** utilizando tecnolog√≠as de Google Cloud Platform y LangGraph. El agente puede responder preguntas sobre Punta Blanca Solutions bas√°ndose en informaci√≥n extra√≠da de su sitio web y LinkedIn.

### üéØ Objetivo Principal
Crear un agente de IA que pueda:
1. **Recibir preguntas** a trav√©s de una API REST
2. **Buscar informaci√≥n relevante** en una base de conocimiento vectorial
3. **Generar respuestas** utilizando un LLM con el contexto recuperado
4. **Responder en formato JSON estructurado** con fuentes y nivel de confianza

## üèóÔ∏è Arquitectura del Sistema

### Arquitectura General
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Input Node    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Retrieval Node  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Generation Node ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Output Node    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ Validaci√≥n    ‚îÇ    ‚îÇ ‚Ä¢ Embeddings     ‚îÇ    ‚îÇ ‚Ä¢ LLM Prompt    ‚îÇ    ‚îÇ ‚Ä¢ Formato JSON  ‚îÇ
‚îÇ ‚Ä¢ Limpieza      ‚îÇ    ‚îÇ ‚Ä¢ Pinecone Query ‚îÇ    ‚îÇ ‚Ä¢ Context Build ‚îÇ    ‚îÇ ‚Ä¢ Respuesta     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos
1. **Recepci√≥n**: API REST recibe pregunta del usuario
2. **Validaci√≥n**: Se valida y limpia la pregunta
3. **B√∫squeda**: Se genera embedding y se busca en Pinecone
4. **Generaci√≥n**: Se construye contexto y se genera respuesta con LLM
5. **Formato**: Se estructura la respuesta en JSON con metadatos

## üõ†Ô∏è Stack Tecnol√≥gico

### Tecnolog√≠as Principales
- **FastAPI**: Framework web moderno y r√°pido para la API REST
- **LangGraph**: Orquestaci√≥n del agente con flujo de trabajo definido
- **OpenAI GPT-4o-mini**: LLM para generaci√≥n de respuestas
- **OpenAI text-embedding-3-small**: Modelo de embeddings optimizado
- **Pinecone**: Base de datos vectorial para b√∫squeda sem√°ntica
- **Python 3.11**: Versi√≥n estable y compatible con todas las dependencias

### Dependencias de Desarrollo
- **Black**: Formateador de c√≥digo Python
- **Ruff**: Linter r√°pido para Python
- **Pytest**: Framework de testing

## üìÅ Estructura del Proyecto

```
ai-agent/

‚îú‚îÄ‚îÄ app/                          # Aplicaci√≥n principal
‚îÇ   ‚îú‚îÄ‚îÄ api/                     # Endpoints de la API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py             # Aplicaci√≥n FastAPI principal
‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Configuraci√≥n central
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py         # Gesti√≥n de variables de entorno
‚îÇ   ‚îî‚îÄ‚îÄ graph/                   # Agente LangGraph
‚îÇ       ‚îî‚îÄ‚îÄ agent_graph.py      # Implementaci√≥n del agente RAG

‚îú‚îÄ‚îÄ data/                        # Datos originales de Punta Blanca
‚îÇ   ‚îú‚îÄ‚îÄ *.json                  # Documentos originales (solo para referencia)
‚îÇ   ‚îú‚îÄ‚îÄ processed_chunks.json   # Chunks de texto procesados
‚îÇ   ‚îî‚îÄ‚îÄ embeddings_processed.json # Embeddings generados

‚îú‚îÄ‚îÄ ingest/                      # Pipeline de procesamiento (ya ejecutado)
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py   # Procesamiento de documentos
‚îÇ   ‚îú‚îÄ‚îÄ embedding_processor.py  # Generaci√≥n de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ pinecone_uploader.py    # Carga a Pinecone
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py         # Orquestador del pipeline

‚îú‚îÄ‚îÄ scripts/                     # Scripts de utilidad
‚îÇ   ‚îî‚îÄ‚îÄ smoke_check.py          # Verificaci√≥n de configuraci√≥n

‚îú‚îÄ‚îÄ Dockerfile                   # Containerizaci√≥n para Cloud Run
‚îú‚îÄ‚îÄ .dockerignore               # Archivos a excluir del Docker
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias de Python
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

## üîß Decisiones T√©cnicas y Justificaciones

### 1. **Elecci√≥n de OpenAI como LLM**
**Decisi√≥n**: Uso de GPT-4o-mini y text-embedding-3-small
**Justificaci√≥n**: 
- **Cr√©ditos existentes**: Ya dispon√≠a de cr√©ditos en OpenAI, representando costo cero
- **Calidad probada**: GPT-4o-mini ofrece excelente relaci√≥n calidad-precio
- **Consistencia**: Mismo proveedor para embeddings y generaci√≥n asegura compatibilidad
- **API estable**: OpenAI tiene una de las APIs m√°s estables y documentadas

### 2. **Modelo de Embeddings Small**
**Decisi√≥n**: text-embedding-3-small en lugar de text-embedding-3-large
**Justificaci√≥n**:
- **Optimizaci√≥n de costos**: 50% menos costoso que el modelo large
- **Dimensiones adecuadas**: 1536 dimensiones son suficientes para b√∫squeda sem√°ntica
- **Performance**: Mantiene excelente calidad para casos de uso de RAG
- **Velocidad**: Generaci√≥n m√°s r√°pida de embeddings

### 3. **Tama√±o de Chunks Optimizado**
**Decisi√≥n**: Chunk size de 750 caracteres con overlap de 150
**Justificaci√≥n**:
- **Contexto preservado**: 750 caracteres permiten mantener oraciones completas
- **Overlap estrat√©gico**: 150 caracteres aseguran continuidad entre chunks
- **Balance memoria-calidad**: Optimiza uso de tokens del LLM
- **Evita cortes abruptos**: Previene p√©rdida de contexto importante

### 4. **Arquitectura de Carpetas data/ingest**
**Decisi√≥n**: Separaci√≥n clara entre datos crudos y pipeline de procesamiento
**Justificaci√≥n**:
- **Datos estructurados**: Los JSONs contienen informaci√≥n limpia y organizada
- **Pipeline reutilizable**: El proceso de ingest puede aplicarse a nuevos datos
- **Separaci√≥n de responsabilidades**: Datos vs. l√≥gica de procesamiento
- **Facilita mantenimiento**: Estructura clara para futuras expansiones

### 5. **Uso de Pinecone como Vector Store**
**Decisi√≥n**: Pinecone en lugar de alternativas gratuitas como Chroma o FAISS
**Justificaci√≥n**:
- **Escalabilidad**: Maneja grandes vol√∫menes de vectores eficientemente
- **Reranking avanzado**: Implementa BGE-reranker-v2-m3 para mejor relevancia
- **API robusta**: Interfaz estable y bien documentada
- **Integraci√≥n nativa**: Funciona perfectamente con LangChain/LangGraph

### 6. **Implementaci√≥n de Reranking**
**Decisi√≥n**: Uso de BGE-reranker-v2-m3 en Pinecone
**Justificaci√≥n**:
- **Mejora significativa**: Reranking puede mejorar la relevancia en 20-30%
- **Costo-beneficio**: El costo adicional se compensa con mejor calidad de respuestas
- **Implementaci√≥n nativa**: Pinecone lo maneja autom√°ticamente
- **Sin complejidad adicional**: No requiere l√≥gica adicional en el c√≥digo

### 7. **Arquitectura LangGraph con 4 Nodos**
**Decisi√≥n**: Implementaci√≥n secuencial simple en lugar de flujo complejo
**Justificaci√≥n**:
- **Simplicidad**: Soluci√≥n que funciona vs. complejidad innecesaria
- **Debugging f√°cil**: Cada nodo tiene responsabilidad √∫nica
- **Mantenibilidad**: F√°cil de entender y modificar
- **Escalabilidad**: F√°cil agregar nuevos nodos o modificar flujo

### 8. **FastAPI como Framework Web**
**Decisi√≥n**: FastAPI en lugar de Flask o Django
**Justificaci√≥n**:
- **Performance**: Rendimiento superior para APIs
- **Type hints**: Validaci√≥n autom√°tica con Pydantic
- **Documentaci√≥n autom√°tica**: Swagger/OpenAPI generado autom√°ticamente
- **Async support**: Preparado para operaciones as√≠ncronas futuras

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos
- Python 3.11+
- Docker (para containerizaci√≥n)
- Cuenta en OpenAI con API key
- Cuenta en Pinecone con API key

### Variables de Entorno
Crear archivo `.env` en la ra√≠z del proyecto:
```bash
# OpenAI Configuration
OPENAI_API_KEY=tu_api_key_aqui
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Pinecone Configuration
PINECONE_API_KEY=tu_api_key_aqui
PINECONE_INDEX=agent-db
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Application Configuration
ENV=dev
```

### Instalaci√≥n Local
```bash
# Clonar repositorio
git clone https://github.com/[tu-usuario]/ai-agent-pbsolutions.git
cd ai-agent-pbsolutions

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tus API keys

# Ejecutar pipeline de ingest (solo la primera vez)
python -m ingest.run_pipeline

# Ejecutar aplicaci√≥n
uvicorn app.api.main:app --reload
```

### Instalaci√≥n con Docker
```bash
# Construir imagen
docker build -t pb-rag .

# Ejecutar contenedor
docker run -p 8080:8080 --env-file .env pb-rag
```

## üìä Pipeline de Datos

### 1. **Procesamiento de Documentos**
```bash
python -m ingest.document_processor
```
- Lee archivos JSON de la carpeta `data/`
- Divide texto en chunks de 750 caracteres
- Mantiene metadatos (fuente, secci√≥n, ID)

### 2. **Generaci√≥n de Embeddings**
```bash
python -m ingest.embedding_processor
```
- Genera embeddings para cada chunk
- Usa OpenAI text-embedding-3-small
- Almacena embeddings en formato JSON

### 3. **Carga a Pinecone**
```bash
python -m ingest.pinecone_uploader
```
- Carga embeddings a Pinecone en lotes
- Configura √≠ndice con dimensiones correctas
- Verifica carga exitosa

### 4. **Pipeline Completo**
```bash
python -m ingest.run_pipeline
```
- Ejecuta todo el proceso secuencialmente
- Proporciona feedback en tiempo real
- Maneja errores y contin√∫a el proceso

## üîç Uso de la API

### Endpoint Principal: `/ask`
**POST** `/ask`

**Request Body:**
```json
{
  "question": "¬øQu√© servicios ofrece Punta Blanca?"
}
```

**Response:**
```json
{
  "answer": "Punta Blanca ofrece servicios de consultor√≠a en IA, desarrollo de soluciones a medida...",
  "sources": [
    "https://www.puntablanca.ai/services",
    "https://www.puntablanca.ai/"
  ],
  "confidence": 0.85
}
```

### Health Check: `/healthz`
**GET** `/healthz`

**Response:**
```json
{
  "status": "ok",
  "env": "dev",
  "openai_model": "gpt-4o-mini",
  "pinecone_index": "agent-db"
}
```

### Ejemplos de Uso

#### Con curl
```bash
curl -X POST "http://localhost:8080/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "¬øQui√©nes son los fundadores de Punta Blanca?"}'
```

#### Con Python
```python
import requests

response = requests.post(
    "http://localhost:8080/ask",
    json={"question": "¬øQu√© es AI Fast Track?"}
)

print(response.json())
```

## üß™ Testing y Verificaci√≥n

### Smoke Test
```bash
python scripts/smoke_check.py
```
Verifica:
- Conexi√≥n con OpenAI
- Conexi√≥n con Pinecone
- Creaci√≥n/configuraci√≥n de √≠ndice
- Operaciones b√°sicas de vectores

### Tests Unitarios
```bash
pytest test/
```

### Verificaci√≥n Manual
1. Ejecutar aplicaci√≥n
2. Hacer pregunta de prueba
3. Verificar respuesta coherente
4. Verificar fuentes y confianza

## üöÄ Deployment en Google Cloud Run

### **Proceso Simplificado (3 pasos):**

#### **Paso 1: Subir Docker a Google Container Registry**
```bash
# Construir imagen con nombre de Google
docker build -t gcr.io/TU_PROJECT_ID/pb-rag:v1 .

# Subir imagen a Google
docker push gcr.io/TU_PROJECT_ID/pb-rag:v1
```

#### **Paso 2: Desplegar en Cloud Run**
```bash
gcloud run deploy pb-rag-api \
    --image gcr.io/TU_PROJECT_ID/pb-rag:v1 \
    --platform managed \
    --region us-central1 \
    --allow-unauthenticated \
    --port 8080
```

#### **Paso 3: Configurar Variables de Entorno en Google Cloud**
- Ir a Google Cloud Console > Cloud Run > tu servicio
- En "Variables & Secrets" agregar:
  - `OPENAI_API_KEY`: Tu API key de OpenAI
  - `PINECONE_API_KEY`: Tu API key de Pinecone
  - `PINECONE_INDEX`: Nombre de tu √≠ndice Pinecone

### **Verificar Deployment**
```bash
# Obtener URL del servicio
gcloud run services describe pb-rag-api --region=us-central1

# Probar endpoint
curl -X POST "[URL]/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "Test question"}'
```

## üìö Recursos y Referencias

### Documentaci√≥n Oficial
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Pinecone Documentation](https://docs.pinecone.io/)

### Tutoriales y Ejemplos
- [LangGraph Agent Examples](https://github.com/langchain-ai/langgraph/tree/main/examples)
- [FastAPI Deployment Guide](https://fastapi.tiangolo.com/deployment/)
- [Google Cloud Run Quickstart](https://cloud.google.com/run/docs/quickstarts)

### Herramientas de Desarrollo
- [Postman](https://www.postman.com/) - Testing de API
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) - Containerizaci√≥n
- [Google Cloud CLI](https://cloud.google.com/sdk/docs/install) - Deployment

## ü§ù Contribuci√≥n

### Estructura de Commits
- `feat:` Nueva funcionalidad
- `fix:` Correcci√≥n de bugs
- `docs:` Documentaci√≥n
- `refactor:` Refactorizaci√≥n de c√≥digo
- `test:` Tests y testing

### Pull Request Process
1. Fork del repositorio
2. Crear rama feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -m 'feat: agregar nueva funcionalidad'`
4. Push a rama: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

## üìÑ Licencia

Este proyecto es parte de una prueba t√©cnica para el puesto de AI Engineer en Punta Blanca Solutions.

## üë®‚Äçüíª Autor

Desarrollado como prueba t√©cnica para demostrar capacidades en:
- Implementaci√≥n de RAG con LangGraph
- Integraci√≥n de APIs de IA (OpenAI, Pinecone)
- Desarrollo de APIs REST con FastAPI
- Containerizaci√≥n y deployment en Google Cloud Platform
- Arquitectura de sistemas de IA escalables

---

**Nota**: Este proyecto est√° optimizado para funcionar dentro de los l√≠mites gratuitos de GCP y utiliza eficientemente los cr√©ditos disponibles en OpenAI para minimizar costos operativos.
